{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM46fKgkwj6Qpxm5VfXMYQa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sornambal/PGAGI-ASSIGNMENT---TalentScout-AI-Hiring-Assistant-Chatbot/blob/main/AIML_INTERN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7HhPnpzu-HH3"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet gradio openai cryptography\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, getpass\n",
        "\n",
        "# OPTION 1: Hide input (recommended)\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key (or leave blank for fallback): \")\n",
        "\n",
        "# OPTION 2 (alternative): Paste directly (not recommended if sharing notebook)\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"sk-your-api-key\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZaEZlqKMsz1",
        "outputId": "89722f77-709d-4741-9e5c-4e3cd1e0ff73"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key (or leave blank for fallback): ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re, os, json, sqlite3\n",
        "from datetime import datetime\n",
        "from cryptography.fernet import Fernet\n",
        "import openai\n",
        "\n",
        "# Get API Key\n",
        "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "if OPENAI_KEY:\n",
        "    openai.api_key = OPENAI_KEY\n",
        "\n",
        "# Encryption key setup\n",
        "FERNET_FILE = \"fernet.key\"\n",
        "if not os.path.exists(FERNET_FILE):\n",
        "    key = Fernet.generate_key()\n",
        "    with open(FERNET_FILE, \"wb\") as f:\n",
        "        f.write(key)\n",
        "else:\n",
        "    with open(FERNET_FILE, \"rb\") as f:\n",
        "        key = f.read()\n",
        "fernet = Fernet(key)\n"
      ],
      "metadata": {
        "id": "QteQxM9VM0Wf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_db():\n",
        "    conn = sqlite3.connect(\"talentscout.db\", check_same_thread=False)\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS candidates (\n",
        "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        created_at TEXT,\n",
        "        name BLOB,\n",
        "        email BLOB,\n",
        "        phone BLOB,\n",
        "        experience TEXT,\n",
        "        position TEXT,\n",
        "        location TEXT,\n",
        "        tech_stack TEXT,\n",
        "        questions TEXT\n",
        "    )\n",
        "    \"\"\")\n",
        "    conn.commit()\n",
        "    return conn\n",
        "\n",
        "conn = init_db()\n",
        "\n",
        "def encrypt(value: str):\n",
        "    return fernet.encrypt(value.encode()) if value else None\n",
        "\n",
        "def decrypt(value: bytes):\n",
        "    return fernet.decrypt(value).decode() if value else \"\"\n"
      ],
      "metadata": {
        "id": "TWAjkVi8M3Bl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_email(email): return re.match(r\"^[^@]+@[^@]+\\.[^@]+$\", email)\n",
        "def validate_phone(phone): return re.match(r\"^\\+?\\d[\\d\\-\\s]{6,}$\", phone)\n",
        "\n",
        "def save_candidate(data):\n",
        "    c = conn.cursor()\n",
        "    c.execute(\"\"\"\n",
        "    INSERT INTO candidates (created_at, name, email, phone, experience, position, location, tech_stack, questions)\n",
        "    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "    \"\"\", (\n",
        "        datetime.utcnow().isoformat(),\n",
        "        encrypt(data.get(\"name\")),\n",
        "        encrypt(data.get(\"email\")),\n",
        "        encrypt(data.get(\"phone\")),\n",
        "        data.get(\"experience\"),\n",
        "        data.get(\"position\"),\n",
        "        data.get(\"location\"),\n",
        "        json.dumps(data.get(\"tech_stack\", [])),\n",
        "        json.dumps(data.get(\"questions\", {}))\n",
        "    ))\n",
        "    conn.commit()\n",
        "    return c.lastrowid\n",
        "\n",
        "def parse_stack(stack_str):\n",
        "    return [s.strip() for s in re.split(r\"[,;\\n]+\", stack_str) if s.strip()]\n",
        "\n",
        "def get_openai_questions(tech, years, position, count=4):\n",
        "    prompt = f\"\"\"\n",
        "    Generate {count} concise technical interview questions for {tech}.\n",
        "    Candidate: {years} years of experience, applying for {position}.\n",
        "    Return as a numbered list.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4o-mini\" if OPENAI_KEY else \"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=400,\n",
        "        )\n",
        "        return response.choices[0].message.content.strip().split(\"\\n\")\n",
        "    except Exception as e:\n",
        "        print(\"‚ö†Ô∏è OpenAI fallback:\", e)\n",
        "        return fallback_questions(tech)\n",
        "\n",
        "def fallback_questions(tech):\n",
        "    base = [\n",
        "        f\"What are key principles of {tech}?\",\n",
        "        f\"Explain common mistakes in {tech}.\",\n",
        "        f\"How would you optimize performance in {tech}?\",\n",
        "        f\"Describe a real-world use case of {tech}.\"\n",
        "    ]\n",
        "    if \"python\" in tech.lower():\n",
        "        base += [\"What is GIL?\", \"Difference between list and tuple in Python?\"]\n",
        "    if \"django\" in tech.lower():\n",
        "        base += [\"Explain Django ORM.\", \"What are middlewares in Django?\"]\n",
        "    if \"react\" in tech.lower():\n",
        "        base += [\"What are hooks in React?\", \"How to optimize rendering?\"]\n",
        "    return base[:4]\n",
        "\n",
        "def generate_questions(stack, years, position):\n",
        "    return {tech: get_openai_questions(tech, years, position) for tech in stack}\n"
      ],
      "metadata": {
        "id": "NmGMqG4LM5YW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "session = {\"data\": {}, \"questions\": {}}\n",
        "END_WORDS = {\"end\", \"exit\", \"bye\", \"stop\", \"quit\"}\n",
        "\n",
        "def reset():\n",
        "    session[\"data\"] = {\n",
        "        \"name\": None, \"email\": None, \"phone\": None,\n",
        "        \"experience\": None, \"position\": None,\n",
        "        \"location\": None, \"tech_stack\": None\n",
        "    }\n",
        "    session[\"questions\"] = {}\n",
        "    return \"üëã Hi, I‚Äôm TalentScout Hiring Assistant! Let‚Äôs begin.\\nPlease tell me your *full name*.\"\n",
        "\n",
        "def handle(user_input):\n",
        "    text = user_input.strip()\n",
        "\n",
        "    # Exit handling\n",
        "    if text.lower() in END_WORDS:\n",
        "        if session[\"data\"].get(\"name\"):\n",
        "            save_candidate({**session[\"data\"], \"questions\": session[\"questions\"]})\n",
        "            return \"‚úÖ Thank you! Your responses are saved. We‚Äôll contact you soon.\"\n",
        "        return \"Goodbye!\"\n",
        "\n",
        "    # --- üß† Smart Name Detection ---\n",
        "    if not session[\"data\"].get(\"name\"):\n",
        "        lower = text.lower()\n",
        "        # If user says \"I am ...\" or \"My name is ...\"\n",
        "        if \"my name is\" in lower or \"i am\" in lower:\n",
        "            # Extract words after \"is\" or \"am\"\n",
        "            parts = lower.split(\"is\")[-1] if \"is\" in lower else lower.split(\"am\")[-1]\n",
        "            name = parts.strip().title()\n",
        "            session[\"data\"][\"name\"] = name\n",
        "            return f\"Nice to meet you, {name}! Please provide your *email address*.\"\n",
        "        # If they just typed a short name (<=3 words)\n",
        "        elif len(text.split()) <= 3:\n",
        "            session[\"data\"][\"name\"] = text.title()\n",
        "            return f\"Thanks, {session['data']['name']}! Please provide your *email address*.\"\n",
        "\n",
        "    # --- üß© Handle colon-style structured inputs (name:, email:, etc.) ---\n",
        "    if \":\" in text:\n",
        "        key, val = [x.strip() for x in text.split(\":\", 1)]\n",
        "        key_lower = key.lower()\n",
        "\n",
        "        if \"name\" in key_lower:\n",
        "            session[\"data\"][\"name\"] = val.title()\n",
        "            return f\"Got it, {session['data']['name']}. Please share your *email address*.\"\n",
        "        elif \"email\" in key_lower:\n",
        "            if validate_email(val):\n",
        "                session[\"data\"][\"email\"] = val\n",
        "                return \"Email saved. Please provide your *phone number* (include country code).\"\n",
        "            else:\n",
        "                return \"‚ùå Invalid email. Try again.\"\n",
        "        elif \"phone\" in key_lower:\n",
        "            if validate_phone(val):\n",
        "                session[\"data\"][\"phone\"] = val\n",
        "                return \"Great! How many *years of experience* do you have?\"\n",
        "            else:\n",
        "                return \"‚ùå Invalid phone number.\"\n",
        "        elif \"experience\" in key_lower:\n",
        "            session[\"data\"][\"experience\"] = val\n",
        "            return \"Nice! What *position* are you applying for?\"\n",
        "        elif \"position\" in key_lower:\n",
        "            session[\"data\"][\"position\"] = val\n",
        "            return \"Understood. Where are you *currently located*?\"\n",
        "        elif \"location\" in key_lower:\n",
        "            session[\"data\"][\"location\"] = val\n",
        "            return \"Thanks! Finally, what‚Äôs your *tech stack*? (e.g., Python, Django, React)\"\n",
        "        elif \"tech\" in key_lower:\n",
        "            stack = parse_stack(val)\n",
        "            session[\"data\"][\"tech_stack\"] = stack\n",
        "            q = generate_questions(\n",
        "                stack,\n",
        "                session[\"data\"].get(\"experience\", \"0\"),\n",
        "                session[\"data\"].get(\"position\", \"Candidate\")\n",
        "            )\n",
        "            session[\"questions\"] = q\n",
        "            return f\"‚úÖ Tech stack saved: {', '.join(stack)}.\\nI‚Äôve generated questions. Type 'show' to view them.\"\n",
        "\n",
        "    # --- üßæ Show questions ---\n",
        "    if text.lower().startswith(\"show\"):\n",
        "        if not session[\"questions\"]:\n",
        "            return \"Please provide your tech stack first.\"\n",
        "        reply = \"\"\n",
        "        for tech, qs in session[\"questions\"].items():\n",
        "            reply += f\"\\n**{tech}**:\\n\" + \"\\n\".join([f\"{i+1}. {q}\" for i, q in enumerate(qs)]) + \"\\n\"\n",
        "        return reply\n",
        "\n",
        "    # --- üß© Missing info fallback prompts ---\n",
        "    missing = [k for k, v in session[\"data\"].items() if not v]\n",
        "    if missing:\n",
        "        prompts = {\n",
        "            \"name\": \"Please tell me your *full name*.\",\n",
        "            \"email\": \"Please provide your *email address*.\",\n",
        "            \"phone\": \"Please provide your *phone number* (include country code).\",\n",
        "            \"experience\": \"Please specify your *years of experience*.\",\n",
        "            \"position\": \"Which *position* are you applying for?\",\n",
        "            \"location\": \"Where are you *currently located*?\",\n",
        "            \"tech_stack\": \"What‚Äôs your *tech stack*? (e.g., Python, Django, React)\"\n",
        "        }\n",
        "        return prompts[missing[0]]\n",
        "\n",
        "    # --- üèÅ Final step ---\n",
        "    return \"Type 'show' to see questions or 'end' to finish the conversation.\"\n"
      ],
      "metadata": {
        "id": "Fm3OqvHeM9eP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def validate_email(email: str) -> bool:\n",
        "    \"\"\"Check if the email format is valid\"\"\"\n",
        "    return bool(re.match(r\"^[^@]+@[^@]+\\.[^@]+$\", email.strip()))\n",
        "\n",
        "def validate_phone(phone: str) -> bool:\n",
        "    \"\"\"Check if the phone number looks valid (digits and optional +country code)\"\"\"\n",
        "    return bool(re.match(r\"^\\+?\\d[\\d\\-\\s]{6,}$\", phone.strip()))\n",
        "\n",
        "def parse_stack(stack_str: str):\n",
        "    \"\"\"Split comma or semicolon separated tech stack\"\"\"\n",
        "    return [s.strip() for s in re.split(r\"[,;\\n]+\", stack_str) if s.strip()]\n"
      ],
      "metadata": {
        "id": "P-5ivufHQQXJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, re\n",
        "import openai\n",
        "import os\n",
        "\n",
        "# ‚úÖ Make sure OpenAI key is set (optional)\n",
        "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "if OPENAI_KEY:\n",
        "    openai.api_key = OPENAI_KEY\n",
        "\n",
        "# üß† Fallback question generator for offline or keyless mode\n",
        "def fallback_questions(tech):\n",
        "    \"\"\"Return some basic predefined questions for known tech stacks\"\"\"\n",
        "    tech_lower = tech.lower()\n",
        "    base = [\n",
        "        f\"What are key principles of {tech}?\",\n",
        "        f\"Explain common mistakes in {tech}.\",\n",
        "        f\"How would you optimize performance in {tech}?\",\n",
        "        f\"Describe a real-world use case of {tech}.\"\n",
        "    ]\n",
        "    if \"python\" in tech_lower:\n",
        "        base += [\"Explain Python‚Äôs GIL.\", \"Difference between list and tuple.\"]\n",
        "    if \"django\" in tech_lower:\n",
        "        base += [\"What is Django ORM?\", \"Explain Django middleware.\"]\n",
        "    if \"react\" in tech_lower:\n",
        "        base += [\"What are React hooks?\", \"How does the virtual DOM work?\"]\n",
        "    if \"node\" in tech_lower:\n",
        "        base += [\"Explain the event loop in Node.js.\", \"How do you handle async errors?\"]\n",
        "    return base[:4]\n",
        "\n",
        "# ‚úÖ OpenAI question generator (if API key available)\n",
        "def get_openai_questions(tech, years, position, count=4):\n",
        "    prompt = f\"\"\"\n",
        "    Generate {count} concise technical interview questions for {tech}.\n",
        "    Candidate has {years} years of experience applying for {position}.\n",
        "    Return a simple numbered list (no answers).\n",
        "    \"\"\"\n",
        "    if not OPENAI_KEY:\n",
        "        return fallback_questions(tech)\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=400,\n",
        "        )\n",
        "        output = response.choices[0].message.content.strip()\n",
        "        lines = [l.strip(\"-‚Ä¢ \") for l in output.splitlines() if l.strip()]\n",
        "        return lines[:count] or fallback_questions(tech)\n",
        "    except Exception as e:\n",
        "        print(\"‚ö†Ô∏è OpenAI error:\", e)\n",
        "        return fallback_questions(tech)\n",
        "\n",
        "# ‚úÖ Main function that your chatbot calls\n",
        "def generate_questions(stack, years, position):\n",
        "    \"\"\"Generate a dictionary of {tech: [questions]}\"\"\"\n",
        "    results = {}\n",
        "    for tech in stack:\n",
        "        results[tech] = get_openai_questions(tech, years, position)\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "HHb0mfaLRPWs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as app:\n",
        "    gr.Markdown(\"# ü§ñ TalentScout Hiring Assistant Chatbot\")\n",
        "    chat = gr.Chatbot()\n",
        "    msg = gr.Textbox(placeholder=\"Type here... (e.g., 'email: you@example.com')\", lines=2)\n",
        "    btn = gr.Button(\"Send\")\n",
        "    clear = gr.Button(\"Reset Chat\")\n",
        "\n",
        "    state = gr.State([])\n",
        "\n",
        "    def chat_flow(message, history):\n",
        "        reply = handle(message)\n",
        "        history.append((message, reply))\n",
        "        return \"\", history\n",
        "\n",
        "    def clear_chat():\n",
        "        reset()\n",
        "        return [], \"Session reset.\"\n",
        "\n",
        "    btn.click(chat_flow, [msg, chat], [msg, chat])\n",
        "    clear.click(lambda: reset(), None, chat)\n",
        "\n",
        "reset()\n",
        "app.launch(debug=True, share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 993
        },
        "id": "6ZOtLSlAM__U",
        "outputId": "614ba515-2a2c-4749-d1de-1c76a7a8efb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1948965398.py:3: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chat = gr.Chatbot()\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://7fa3bb50d34763b1ab.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://7fa3bb50d34763b1ab.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è OpenAI error: \n",
            "\n",
            "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
            "\n",
            "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
            "\n",
            "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
            "\n",
            "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
            "\n",
            "‚ö†Ô∏è OpenAI error: \n",
            "\n",
            "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
            "\n",
            "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
            "\n",
            "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
            "\n",
            "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
            "\n"
          ]
        }
      ]
    }
  ]
}